{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JhHukHgpFgxi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f95810e-cab6-48b1-e439-063b3dfc6a9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
            "Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.0\n",
            "Collecting accelerate\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.17.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (3.27.6)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.10.0->accelerate) (17.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: accelerate\n",
            "Successfully installed accelerate-0.23.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install accelerate\n",
        "# TODO: check lr scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ALCt2-aGrI0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import cohen_kappa_score as kappa\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import GPT2Tokenizer, GPT2Model\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import time\n",
        "import pathlib\n",
        "\n",
        "# log folder to save log files\n",
        "log_folder = '/content/drive/MyDrive/asap/'\n",
        "\n",
        "# target column\n",
        "target_column = \"score\"\n",
        "\n",
        "# hyper parameters\n",
        "hp = {\n",
        "    \"base_model\": \"gpt2\",\n",
        "    \"lr\": 1e-4,\n",
        "    \"num_epochs\": 25,\n",
        "    \"batch_size\": 2,\n",
        "    \"use_amp\": True,\n",
        "    \"mixed_precision\": \"fp16\",\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5E6LL6CGlGU"
      },
      "source": [
        "# Prepare ASAP Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-UI3sfsWEZi",
        "outputId": "7d32c866-b8fa-4e29-967d-35058f4ee174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iI4gDr87GohH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "2ef0bfbc-49a6-4cb4-b4f9-46f5316f4a3a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   essay_id  essay_set                                              essay  \\\n",
              "0         1          1  Dear local newspaper, I think effects computer...   \n",
              "1         2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
              "2         3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
              "3         4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
              "4         5          1  Dear @LOCATION1, I know having computers has a...   \n",
              "\n",
              "   rater1  rater2  score  \n",
              "0       4       4      8  \n",
              "1       5       4      9  \n",
              "2       4       3      7  \n",
              "3       5       5     10  \n",
              "4       4       4      8  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8a00d862-fbd4-4c62-821c-056f6a3423d4\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay_id</th>\n",
              "      <th>essay_set</th>\n",
              "      <th>essay</th>\n",
              "      <th>rater1</th>\n",
              "      <th>rater2</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear local newspaper, I think effects computer...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>1</td>\n",
              "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a00d862-fbd4-4c62-821c-056f6a3423d4')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8a00d862-fbd4-4c62-821c-056f6a3423d4 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8a00d862-fbd4-4c62-821c-056f6a3423d4');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e6d770c7-cef2-4518-ae38-f18f6bf17665\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e6d770c7-cef2-4518-ae38-f18f6bf17665')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e6d770c7-cef2-4518-ae38-f18f6bf17665 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Original kaggle training set\n",
        "kaggle_dataset  = pd.read_csv('/content/drive/MyDrive/GPT-2 MODEL/training_set_rel3.tsv', sep='\\t', encoding = \"ISO-8859-1\")\n",
        "\n",
        "# Smaller training set used for this project\n",
        "dataset_df = pd.DataFrame(\n",
        "  {\n",
        "    'essay_id' : kaggle_dataset['essay_id'],\n",
        "    'essay_set' : kaggle_dataset['essay_set'],\n",
        "    'essay' : kaggle_dataset['essay'],\n",
        "    'rater1' : kaggle_dataset['rater1_domain1'],\n",
        "    'rater2' : kaggle_dataset['rater2_domain1'],\n",
        "    'score' : kaggle_dataset['domain1_score']\n",
        "  })\n",
        "dataset_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uTmNpJqfHY2R"
      },
      "source": [
        "## Use essay_set=2 for classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Il-xAB2lHVvq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9e15363-152b-46e2-f17a-112e325d9628"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1800, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "essay_df = dataset_df[dataset_df['essay_set'] == 2].copy()\n",
        "essay_df.shape\n",
        "\n",
        "# essay_df = dataset_df.loc[(dataset_df['essay_set'] == 3) | (dataset_df['essay_set'] == 4) | (dataset_df['essay_set'] == 5) | (dataset_df['essay_set'] == 6)].copy()\n",
        "# essay_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYnqfTkQxBzr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Use minimum score\n",
        "rater_1 = essay_df[\"rater1\"].to_numpy()\n",
        "rater_2 = essay_df[\"rater2\"].to_numpy()\n",
        "\n",
        "min_score = np.minimum(rater_1, rater_2)\n",
        "max_score = np.maximum(rater_1, rater_2)\n",
        "\n",
        "essay_df['min_score'] = min_score\n",
        "essay_df['max_score'] = max_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5iw8IEz3CTL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef723064-a3d2-4e4e-e0a7-58e907a94843"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    778\n",
              "3    763\n",
              "2    153\n",
              "5     75\n",
              "1     24\n",
              "6      7\n",
              "Name: score, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "essay_df['score'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essay_df['score'] = essay_df['score'].astype(int)\n"
      ],
      "metadata": {
        "id": "F7eEQQh4LjhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install googletrans==4.0.0-rc1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "qDpmkDC5LIo1",
        "outputId": "ccbd2c49-517d-4984-92d1-7ec60f5d9463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2023.7.22)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2023.1.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17396 sha256=37ed711ef18ce02aec4c2c13614dcecfb2da91bdb64ae9001b57dcce03d52a65\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.4\n",
            "    Uninstalling idna-3.4:\n",
            "      Successfully uninstalled idna-3.4\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2023.1.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "chardet",
                  "idna"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from googletrans import Translator\n",
        "import pandas as pd\n",
        "\n",
        "def back_translate(text):\n",
        "    translator = Translator()\n",
        "    target_language = 'tr'  # set the target language directly as Turkish\n",
        "\n",
        "    # Translate from English to the target language\n",
        "    translated_text = translator.translate(text, dest=target_language).text\n",
        "\n",
        "    # Back-translate from the target language to English\n",
        "    back_translated_text = translator.translate(translated_text, dest='en').text\n",
        "\n",
        "    return back_translated_text\n",
        "\n",
        "def augment_data(df, column, value, fraction):\n",
        "    subset = df[df[column] == value]\n",
        "    augmented_subset = subset.sample(frac=fraction, replace=True)\n",
        "\n",
        "    new_rows = []\n",
        "    max_essay_id = df['essay_id'].max()\n",
        "\n",
        "    for _, row in augmented_subset.iterrows():\n",
        "        max_essay_id += 1\n",
        "        new_rows.append({\n",
        "            'essay_id': max_essay_id,\n",
        "            'essay_set': row['essay_set'],\n",
        "            'essay': back_translate(row['essay']),\n",
        "            'rater1': row['rater1'],\n",
        "            'rater2': row['rater2'],\n",
        "            'score': value\n",
        "        })\n",
        "\n",
        "    return pd.DataFrame(new_rows)\n",
        "\n",
        "# Apply data augmentation to classes in essay_df that have less than 50 samples\n",
        "classes_to_augment = [1, 6]\n",
        "augmented_dataframes = []\n",
        "\n",
        "for value in classes_to_augment:\n",
        "    augmented_dataframes.append(augment_data(essay_df, 'score', value, 1.0))\n",
        "\n",
        "augmented_df = pd.concat(augmented_dataframes, ignore_index=True)\n",
        "\n",
        "#Add the augmented data to the original data.\n",
        "essay_df = pd.concat([essay_df, augmented_df], ignore_index=True)\n",
        "\n",
        "print(f\"Total data size after augmentation: {len(essay_df)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5peKUVyLFJ5",
        "outputId": "30e47f10-de8d-43e0-d3f9-1dcc1e61004b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total data size after augmentation: 1831\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essay_df['score'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3us6c77NLfn3",
        "outputId": "fba445f6-92f0-40e5-876b-e49c4aee65ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4    778\n",
              "3    763\n",
              "2    153\n",
              "5     75\n",
              "1     48\n",
              "6     14\n",
              "Name: score, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "essay_df[target_column] - essay_df[target_column].min()"
      ],
      "metadata": {
        "id": "5tSwyjSIMDEr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eeec903-f14a-4990-8721-684b8365b7e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       3\n",
              "1       0\n",
              "2       1\n",
              "3       3\n",
              "4       3\n",
              "       ..\n",
              "1826    5\n",
              "1827    5\n",
              "1828    5\n",
              "1829    5\n",
              "1830    5\n",
              "Name: score, Length: 1831, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w5oZj0cani-Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "essay_df['target_score'] = essay_df[target_column] - essay_df[target_column].min()\n",
        "\n",
        "X, y = essay_df['essay'].to_list(), essay_df['target_score'].to_numpy()\n",
        "num_labels = essay_df[target_column].unique().size\n",
        "\n",
        "# 60 / 40 train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.40, random_state=42) # stratify=y, this paramter will not work if any class has number of examples lower than 2\n",
        "\n",
        "# split test to half to get 60 / 20 / 20 split\n",
        "\n",
        "X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, test_size=0.50, random_state=42) # stratify=y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eng-OPm1Oavu"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# a torch dataset implementation for ASAP dataset\n",
        "class EssayDataset(Dataset):\n",
        "    def __init__(self, essays, targets, tokenizer):\n",
        "        self.essays = essays\n",
        "        self.targets = targets\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.essays)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.essays[idx])\n",
        "        encoded_input = tokenizer(text, truncation=True, return_tensors='pt').to(device)\n",
        "\n",
        "        return encoded_input['input_ids'].squeeze(), encoded_input['attention_mask'].squeeze(), self.targets[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkMVRTCKOWly"
      },
      "outputs": [],
      "source": [
        "# collater function to pad tokens\n",
        "def collate_fn(batch):\n",
        "    PAD_TOKEN_ID = 50256 # Use tokenizer.pad_token_id to check\n",
        "    input_ids_list, attention_mask_list, targets = [], [], []\n",
        "\n",
        "    for input_ids, attention_mask, target in batch:\n",
        "        input_ids_list.append(input_ids)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "        targets.append(target)\n",
        "\n",
        "    # Pad the batch to the maximum sequence length within that batch using the tokenizer's pad token\n",
        "    max_length = max(len(ids) for ids in input_ids_list)\n",
        "    padded_input_ids = []\n",
        "    padded_attention_mask = []\n",
        "\n",
        "    for input_ids, attention_mask in zip(input_ids_list, attention_mask_list):\n",
        "        pad_length = max_length - len(input_ids)\n",
        "        padded_input_ids.append(torch.cat([input_ids, torch.tensor([PAD_TOKEN_ID] * pad_length, device=device, dtype=torch.long)]))\n",
        "        # add zeros to attention mask for padds\n",
        "        padded_attention_mask.append(torch.cat([attention_mask, torch.zeros(pad_length, dtype=torch.long, device=device)]))\n",
        "\n",
        "    return torch.stack(padded_input_ids), torch.stack(padded_attention_mask), torch.tensor(targets)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0AFe3puzOg9r"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(hp['base_model'])\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTB-6eCBHbB3"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2ForSequenceClassification\n",
        "\n",
        "class ClassifierLayer(torch.nn.Module):\n",
        "\n",
        "  def __init__(self, input_size, output_size, bias=False):\n",
        "    super(ClassifierLayer, self).__init__()\n",
        "\n",
        "    self.dropout = torch.nn.Dropout(0.5)\n",
        "    self.linear = torch.nn.Linear(input_size, output_size, bias=bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    inputs = self.dropout(x)\n",
        "    return self.linear(inputs)\n",
        "\n",
        "class GPT2Classification(GPT2ForSequenceClassification):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.score = ClassifierLayer(config.n_embd, self.num_labels, bias=False)\n",
        "\n",
        "        self.post_init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLqTQ2PRqX12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36e56628-f4ea-46c8-9ffb-25b5263807e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2Classification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.linear.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from accelerate import Accelerator\n",
        "\n",
        "# use fp16 mixed precision to improve training speed\n",
        "accelerator = Accelerator(mixed_precision=hp['mixed_precision']) # fp16\n",
        "device = accelerator.device\n",
        "\n",
        "model = GPT2Classification.from_pretrained(hp['base_model'], num_labels=num_labels)\n",
        "#model = GPT2ForSequenceClassification.from_pretrained(hp['base_model'], num_labels=num_labels)\n",
        "model.to(device)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "# fix model padding token id\n",
        "model.config.pad_token_id = model.config.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMkCwJR5vyKJ"
      },
      "outputs": [],
      "source": [
        "# train loop\n",
        "\n",
        "def train_loop(model, train_loader, val_loader, loss_fct, optimizer, lr_scheduler, progress_bar, log_file_handler, logging_step=1, use_amp=False):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "\n",
        "    # set model to train mode\n",
        "    model.train()\n",
        "\n",
        "    for step, (inputs, attention_masks, targets) in enumerate(train_loader):\n",
        "        targets = targets.reshape(-1, 1).to(device)\n",
        "        attention_masks = attention_masks.to(device)\n",
        "        outputs = model(inputs, attention_mask=attention_masks)\n",
        "        loss = loss_fct(outputs[\"logits\"].view(-1, model.num_labels), targets.view(-1))\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n",
        "\n",
        "        samples += inputs.shape[0]\n",
        "        cumulative_loss += loss.item()\n",
        "\n",
        "        if step % logging_step == 0:\n",
        "            # calculate qwk on test set\n",
        "\n",
        "            with torch.no_grad():\n",
        "                test_loss, test_preds, qwk = test_loop(model, val_loader, loss_fct, use_amp=use_amp)\n",
        "            model.train()\n",
        "            log_str = \"Step: {:<6} \\t Train loss: {:<6.4f} \\t Validation loss: {:<6.4f} \\t QWK: {:<6.4f}\".format(step, (cumulative_loss/samples), test_loss, qwk)\n",
        "            print(log_str)\n",
        "            log_file_handler.write(log_str + \"\\n\")\n",
        "            samples = 0\n",
        "            cumulative_loss = 0\n",
        "\n",
        "    return cumulative_loss/samples if samples != 0 else float(\"inf\")\n",
        "\n",
        "def test_loop(model, test_loader, loss_fct, use_amp=False, show_progression=False):\n",
        "    samples = 0.\n",
        "    cumulative_loss = 0.\n",
        "    preds = []\n",
        "    labels = []\n",
        "\n",
        "    # set model to eval mode\n",
        "    model.eval()\n",
        "\n",
        "    loop_iterator = enumerate(tqdm(test_loader)) if show_progression else enumerate(test_loader)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, (inputs, attention_masks, targets) in loop_iterator:\n",
        "            targets = targets.reshape(-1, 1).to(device)\n",
        "            inputs = inputs.to(device)\n",
        "            attention_masks = attention_masks.to(device)\n",
        "            with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "                outputs = model(inputs, attention_mask=attention_masks)\n",
        "            loss = loss_fct(outputs[\"logits\"].view(-1, model.num_labels), targets.view(-1))\n",
        "\n",
        "            samples += inputs.shape[0]\n",
        "            cumulative_loss += loss.item()\n",
        "\n",
        "            probs = outputs['logits'].softmax(-1) # probs\n",
        "            predictions = probs.argmax(-1) # predicted classes\n",
        "\n",
        "            labels.extend(targets.tolist())\n",
        "            preds.extend(predictions.tolist())\n",
        "\n",
        "        qwk = kappa(preds, labels, weights='quadratic')\n",
        "\n",
        "    return cumulative_loss/samples if samples != 0 else float(\"inf\"), np.asarray(preds, dtype=np.float32), qwk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5B9TGk2Qq0p_"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto  import tqdm\n",
        "from transformers import get_scheduler\n",
        "from torch.optim import AdamW\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "lr = hp['lr']\n",
        "num_epochs = hp['num_epochs']\n",
        "batch_size = hp['batch_size']\n",
        "use_amp = hp['use_amp']\n",
        "\n",
        "# create train data loader\n",
        "train_dataset = EssayDataset(X_train, y_train, tokenizer)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "num_training_steps = num_epochs * len(train_loader)\n",
        "\n",
        "# create test data loader\n",
        "test_dataset = EssayDataset(X_test, y_test, tokenizer)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# create val data loader\n",
        "val_dataset = EssayDataset(X_val, y_val, tokenizer)\n",
        "val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# # get class weights\n",
        "# class_weights = class_weight.compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "# class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "\n",
        "loss_fct = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer,\n",
        "    num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "# use accelerator prepare\n",
        "\n",
        "# removed val_loader from prepare command\n",
        "model, optimizer, train_loader, test_loader, val_loader, lr_scheduler = accelerator.prepare(\n",
        "    model, optimizer, train_loader, test_loader, val_loader, lr_scheduler\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWAD3PIW98zc"
      },
      "outputs": [],
      "source": [
        "def open_log_file(log_folder, essay_df, model, label_column: str, hyper_parameters):\n",
        "    # using time as a file name for logging\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "    file_name = timestr + '.log'\n",
        "\n",
        "    # check if folder exists, create if it isn't\n",
        "    pathlib.Path(log_folder).mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # open file to log results\n",
        "    log_file = os.path.join(log_folder, file_name)\n",
        "    fp = open(log_file, \"a\")\n",
        "\n",
        "    fp.write(\"Log time: \" + timestr + \"\\n\")\n",
        "    fp.write(\"Essay classes: \" + str(essay_df['essay_set'].unique()) + \"\\n\")\n",
        "    fp.write(\"Using score column: \" + label_column + \"\\n\")\n",
        "    fp.write(\"Score distribution: \" + \"\\n\" + essay_df[label_column].value_counts().to_string() + \"\\n\")\n",
        "\n",
        "    fp.write(\"\\n--- Model parameters:\\n\")\n",
        "    fp.write(str(model))\n",
        "    fp.write('\\n')\n",
        "\n",
        "    fp.write(\"\\n--- Hyper parameters:\\n\")\n",
        "    for k, v in hyper_parameters.items():\n",
        "        fp.write(f\" {k:<25}: {v}\\n\")\n",
        "\n",
        "    fp.write('\\n')\n",
        "    fp.flush()\n",
        "    return fp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n"
      ],
      "metadata": {
        "id": "hadgjIQ4ATKa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0769763-4aaa-4663-e413-035bc3be1860"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fri Sep 29 23:49:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   44C    P0    26W /  70W |   1395MiB / 15360MiB |     24%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "hekFmfqJAENE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81b58f3c-5b50-41b7-e8ad-a80071df2adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sti5Zj9TJJnx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "25d7fdf8fd224eadaa9420eceb45f44e",
            "2733d042bb1941109710093679b638e7",
            "3f4d55cd5205405688eb27d63ecdd823",
            "bf9f85a761b744d5b90f3bdd79792f54",
            "0ff14b52dc274d478243f262374b5582",
            "958832d4a620413a81aa962041eb296e",
            "8d13fd0265504eefabb18ed571e6e897",
            "9a8b834322f44b26a77eaf85c397f8f8",
            "162a5efd50254f4e9aebf3fffcb9f7fb",
            "57c3527414e3496b833d9e6e21556ca9",
            "b97816fe717a4867aaf8a3d09023d1d3"
          ]
        },
        "outputId": "c1ecf5c5-01a1-4b84-b6ec-d9018c2bc698"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/13725 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25d7fdf8fd224eadaa9420eceb45f44e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step: 0      \t Train loss: 1.6038 \t Validation loss: 2.0970 \t QWK: 0.0343\n",
            "Step: 183    \t Train loss: 1.5653 \t Validation loss: 0.7772 \t QWK: 0.0068\n",
            "Step: 366    \t Train loss: 0.9088 \t Validation loss: 0.7161 \t QWK: 0.0109\n",
            "Epoch: 1     \t Test  loss: 0.6579 \t QWK: 0.0906\n",
            "Step: 0      \t Train loss: 0.8861 \t Validation loss: 0.6578 \t QWK: 0.0921\n",
            "Step: 183    \t Train loss: 0.7218 \t Validation loss: 0.6416 \t QWK: 0.1594\n",
            "Step: 366    \t Train loss: 0.7037 \t Validation loss: 0.6219 \t QWK: 0.1754\n",
            "Epoch: 2     \t Test  loss: 0.6129 \t QWK: 0.1381\n",
            "Step: 0      \t Train loss: 1.0624 \t Validation loss: 0.6129 \t QWK: 0.1397\n",
            "Step: 183    \t Train loss: 0.6569 \t Validation loss: 0.6024 \t QWK: 0.1739\n",
            "Step: 366    \t Train loss: 0.6581 \t Validation loss: 0.6000 \t QWK: 0.1585\n",
            "Epoch: 3     \t Test  loss: 0.5908 \t QWK: 0.2485\n",
            "Step: 0      \t Train loss: 0.3158 \t Validation loss: 0.5908 \t QWK: 0.2464\n",
            "Step: 183    \t Train loss: 0.6525 \t Validation loss: 0.5885 \t QWK: 0.1786\n",
            "Step: 366    \t Train loss: 0.5934 \t Validation loss: 0.5792 \t QWK: 0.2790\n",
            "Epoch: 4     \t Test  loss: 0.5713 \t QWK: 0.2809\n",
            "Step: 0      \t Train loss: 0.3671 \t Validation loss: 0.5711 \t QWK: 0.2871\n",
            "Step: 183    \t Train loss: 0.5697 \t Validation loss: 0.5598 \t QWK: 0.3178\n",
            "Step: 366    \t Train loss: 0.6295 \t Validation loss: 0.5603 \t QWK: 0.2892\n",
            "Epoch: 5     \t Test  loss: 0.5466 \t QWK: 0.3776\n",
            "Step: 0      \t Train loss: 0.9328 \t Validation loss: 0.5465 \t QWK: 0.3788\n",
            "Step: 183    \t Train loss: 0.5317 \t Validation loss: 0.5378 \t QWK: 0.4039\n",
            "Step: 366    \t Train loss: 0.5554 \t Validation loss: 0.5270 \t QWK: 0.4412\n",
            "Epoch: 6     \t Test  loss: 0.5178 \t QWK: 0.4488\n",
            "Step: 0      \t Train loss: 0.9173 \t Validation loss: 0.5177 \t QWK: 0.4488\n",
            "Step: 183    \t Train loss: 0.5414 \t Validation loss: 0.5234 \t QWK: 0.4411\n",
            "Step: 366    \t Train loss: 0.5115 \t Validation loss: 0.5091 \t QWK: 0.4716\n",
            "Epoch: 7     \t Test  loss: 0.4970 \t QWK: 0.4716\n",
            "Step: 0      \t Train loss: 0.1155 \t Validation loss: 0.4957 \t QWK: 0.4832\n",
            "Step: 183    \t Train loss: 0.4849 \t Validation loss: 0.4838 \t QWK: 0.5311\n",
            "Step: 366    \t Train loss: 0.4659 \t Validation loss: 0.4810 \t QWK: 0.5001\n",
            "Epoch: 8     \t Test  loss: 0.4990 \t QWK: 0.4952\n",
            "Step: 0      \t Train loss: 0.2311 \t Validation loss: 0.5016 \t QWK: 0.4952\n",
            "Step: 183    \t Train loss: 0.4330 \t Validation loss: 0.4985 \t QWK: 0.5148\n",
            "Step: 366    \t Train loss: 0.4248 \t Validation loss: 0.4900 \t QWK: 0.4988\n",
            "Epoch: 9     \t Test  loss: 0.4898 \t QWK: 0.4589\n",
            "Step: 0      \t Train loss: 0.5693 \t Validation loss: 0.4852 \t QWK: 0.4794\n",
            "Step: 183    \t Train loss: 0.4253 \t Validation loss: 0.4958 \t QWK: 0.5089\n",
            "Step: 366    \t Train loss: 0.4279 \t Validation loss: 0.4878 \t QWK: 0.5228\n",
            "Epoch: 10    \t Test  loss: 0.4940 \t QWK: 0.5184\n",
            "Step: 0      \t Train loss: 0.2881 \t Validation loss: 0.4951 \t QWK: 0.5071\n",
            "Step: 183    \t Train loss: 0.3293 \t Validation loss: 0.5366 \t QWK: 0.4984\n",
            "Step: 366    \t Train loss: 0.3687 \t Validation loss: 0.5448 \t QWK: 0.4847\n",
            "Epoch: 11    \t Test  loss: 0.5612 \t QWK: 0.4801\n",
            "Step: 0      \t Train loss: 0.1906 \t Validation loss: 0.5585 \t QWK: 0.4761\n",
            "Step: 183    \t Train loss: 0.2603 \t Validation loss: 0.5689 \t QWK: 0.5338\n",
            "Step: 366    \t Train loss: 0.2500 \t Validation loss: 0.5953 \t QWK: 0.5461\n",
            "Epoch: 12    \t Test  loss: 0.6865 \t QWK: 0.5309\n",
            "Step: 0      \t Train loss: 0.0708 \t Validation loss: 0.6828 \t QWK: 0.5371\n",
            "Step: 183    \t Train loss: 0.2232 \t Validation loss: 0.6318 \t QWK: 0.5965\n",
            "Step: 366    \t Train loss: 0.2526 \t Validation loss: 0.6057 \t QWK: 0.6354\n",
            "Epoch: 13    \t Test  loss: 0.7512 \t QWK: 0.5266\n",
            "Step: 0      \t Train loss: 0.0211 \t Validation loss: 0.7450 \t QWK: 0.5307\n",
            "Step: 183    \t Train loss: 0.2072 \t Validation loss: 0.6510 \t QWK: 0.6443\n",
            "Step: 366    \t Train loss: 0.1898 \t Validation loss: 0.7007 \t QWK: 0.6268\n",
            "Epoch: 14    \t Test  loss: 0.6835 \t QWK: 0.6229\n",
            "Step: 0      \t Train loss: 0.0156 \t Validation loss: 0.6845 \t QWK: 0.6209\n",
            "Step: 183    \t Train loss: 0.1658 \t Validation loss: 0.7410 \t QWK: 0.6414\n",
            "Step: 366    \t Train loss: 0.1540 \t Validation loss: 0.7716 \t QWK: 0.6249\n",
            "Epoch: 15    \t Test  loss: 0.6735 \t QWK: 0.6487\n",
            "Step: 0      \t Train loss: 0.0536 \t Validation loss: 0.6651 \t QWK: 0.6542\n",
            "Step: 183    \t Train loss: 0.1542 \t Validation loss: 0.7421 \t QWK: 0.6504\n",
            "Step: 366    \t Train loss: 0.1390 \t Validation loss: 0.7396 \t QWK: 0.6575\n",
            "Epoch: 16    \t Test  loss: 0.7652 \t QWK: 0.6625\n",
            "Step: 0      \t Train loss: 0.0270 \t Validation loss: 0.7649 \t QWK: 0.6625\n",
            "Step: 183    \t Train loss: 0.1401 \t Validation loss: 0.8271 \t QWK: 0.6409\n",
            "Step: 366    \t Train loss: 0.1347 \t Validation loss: 0.8371 \t QWK: 0.6313\n",
            "Epoch: 17    \t Test  loss: 0.8265 \t QWK: 0.6392\n",
            "Step: 0      \t Train loss: 0.0093 \t Validation loss: 0.8294 \t QWK: 0.6416\n",
            "Step: 183    \t Train loss: 0.1128 \t Validation loss: 0.8295 \t QWK: 0.6456\n",
            "Step: 366    \t Train loss: 0.0965 \t Validation loss: 0.8296 \t QWK: 0.6458\n",
            "Epoch: 18    \t Test  loss: 0.8329 \t QWK: 0.6597\n",
            "Step: 0      \t Train loss: 0.0067 \t Validation loss: 0.8353 \t QWK: 0.6578\n",
            "Step: 183    \t Train loss: 0.0906 \t Validation loss: 0.8551 \t QWK: 0.6539\n",
            "Step: 366    \t Train loss: 0.1348 \t Validation loss: 0.9085 \t QWK: 0.6518\n",
            "Epoch: 19    \t Test  loss: 0.9060 \t QWK: 0.6526\n",
            "Step: 0      \t Train loss: 0.0015 \t Validation loss: 0.9036 \t QWK: 0.6535\n",
            "Step: 183    \t Train loss: 0.0839 \t Validation loss: 0.9877 \t QWK: 0.6465\n",
            "Step: 366    \t Train loss: 0.1195 \t Validation loss: 0.9374 \t QWK: 0.6523\n",
            "Epoch: 20    \t Test  loss: 0.9390 \t QWK: 0.6513\n",
            "Step: 0      \t Train loss: 0.0348 \t Validation loss: 0.9304 \t QWK: 0.6531\n",
            "Step: 183    \t Train loss: 0.0841 \t Validation loss: 0.9608 \t QWK: 0.6451\n",
            "Step: 366    \t Train loss: 0.1053 \t Validation loss: 0.9174 \t QWK: 0.6599\n",
            "Epoch: 21    \t Test  loss: 0.9226 \t QWK: 0.6532\n",
            "Step: 0      \t Train loss: 0.0027 \t Validation loss: 0.9237 \t QWK: 0.6532\n",
            "Step: 183    \t Train loss: 0.0993 \t Validation loss: 0.9294 \t QWK: 0.6546\n",
            "Step: 366    \t Train loss: 0.0760 \t Validation loss: 0.9909 \t QWK: 0.6514\n",
            "Epoch: 22    \t Test  loss: 1.0002 \t QWK: 0.6510\n",
            "Step: 0      \t Train loss: 0.0180 \t Validation loss: 0.9999 \t QWK: 0.6510\n",
            "Step: 183    \t Train loss: 0.0672 \t Validation loss: 0.9760 \t QWK: 0.6541\n",
            "Step: 366    \t Train loss: 0.0785 \t Validation loss: 1.0060 \t QWK: 0.6523\n",
            "Epoch: 23    \t Test  loss: 0.9812 \t QWK: 0.6515\n",
            "Step: 0      \t Train loss: 0.0012 \t Validation loss: 0.9812 \t QWK: 0.6515\n",
            "Step: 183    \t Train loss: 0.0689 \t Validation loss: 0.9902 \t QWK: 0.6533\n",
            "Step: 366    \t Train loss: 0.0704 \t Validation loss: 0.9594 \t QWK: 0.6532\n",
            "Epoch: 24    \t Test  loss: 1.0110 \t QWK: 0.6474\n",
            "Step: 0      \t Train loss: 0.0016 \t Validation loss: 1.0113 \t QWK: 0.6474\n",
            "Step: 183    \t Train loss: 0.0622 \t Validation loss: 0.9996 \t QWK: 0.6482\n",
            "Step: 366    \t Train loss: 0.0716 \t Validation loss: 1.0191 \t QWK: 0.6474\n",
            "Epoch: 25    \t Test  loss: 1.0086 \t QWK: 0.6469\n",
            "Log file closed.\n"
          ]
        }
      ],
      "source": [
        "# Start logging to a file\n",
        "fp = open_log_file(log_folder, essay_df, model, label_column=target_column, hyper_parameters=hp)\n",
        "\n",
        "try:\n",
        "    # Start training\n",
        "    progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "    model.train()\n",
        "    with accelerator.autocast():\n",
        "        fp.write(\"Training logs: \\n\\n\")\n",
        "        for epoch in range(num_epochs):\n",
        "            train_loss = train_loop(model, train_loader, val_loader, loss_fct, optimizer, lr_scheduler, progress_bar, fp, logging_step=len(train_loader)//3, use_amp=hp['use_amp'])\n",
        "            with torch.no_grad():\n",
        "                test_loss, test_preds, qwk = test_loop(model, test_loader, loss_fct)\n",
        "                log_string = \"Epoch: {:<6}\\t Test  loss: {:<6.4f} \\t QWK: {:<6.4f}\".format(epoch+1, test_loss, qwk)\n",
        "                print(log_string)\n",
        "                fp.write(log_string + \"\\n\")\n",
        "finally:\n",
        "    print(\"Log file closed.\")\n",
        "    fp.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "V8VDB6BnLnlg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3b6cd4-9ed1-4304-d6c5-e7b7c5a35671"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2Classification(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D()\n",
              "          (c_proj): Conv1D()\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (score): ClassifierLayer(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (linear): Linear(in_features=768, out_features=4, bias=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3oh_isSwIkf-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "f9746755-1513-428f-ad06-0186b52e38b6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "display = ConfusionMatrixDisplay.from_predictions(y_test, test_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j0iUYxxvKpZU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53adda90-86be-43c8-ed3a-33403357f7fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         2\n",
            "           1       0.17      0.10      0.12        20\n",
            "           2       0.43      0.19      0.27        78\n",
            "           3       0.44      0.71      0.54        80\n",
            "\n",
            "    accuracy                           0.41       180\n",
            "   macro avg       0.26      0.25      0.23       180\n",
            "weighted avg       0.40      0.41      0.37       180\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, test_preds))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "25d7fdf8fd224eadaa9420eceb45f44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2733d042bb1941109710093679b638e7",
              "IPY_MODEL_3f4d55cd5205405688eb27d63ecdd823",
              "IPY_MODEL_bf9f85a761b744d5b90f3bdd79792f54"
            ],
            "layout": "IPY_MODEL_0ff14b52dc274d478243f262374b5582"
          }
        },
        "2733d042bb1941109710093679b638e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_958832d4a620413a81aa962041eb296e",
            "placeholder": "​",
            "style": "IPY_MODEL_8d13fd0265504eefabb18ed571e6e897",
            "value": "100%"
          }
        },
        "3f4d55cd5205405688eb27d63ecdd823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8b834322f44b26a77eaf85c397f8f8",
            "max": 13725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_162a5efd50254f4e9aebf3fffcb9f7fb",
            "value": 13725
          }
        },
        "bf9f85a761b744d5b90f3bdd79792f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57c3527414e3496b833d9e6e21556ca9",
            "placeholder": "​",
            "style": "IPY_MODEL_b97816fe717a4867aaf8a3d09023d1d3",
            "value": " 13725/13725 [21:07&lt;00:00, 16.73it/s]"
          }
        },
        "0ff14b52dc274d478243f262374b5582": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "958832d4a620413a81aa962041eb296e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d13fd0265504eefabb18ed571e6e897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9a8b834322f44b26a77eaf85c397f8f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "162a5efd50254f4e9aebf3fffcb9f7fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "57c3527414e3496b833d9e6e21556ca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b97816fe717a4867aaf8a3d09023d1d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}